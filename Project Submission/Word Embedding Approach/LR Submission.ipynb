{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LR Submission.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"_aEJ8HFy_AVe","colab_type":"code","colab":{}},"source":["import os\n","import re\n","import json\n","import csv\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords \n","from nltk.tokenize import sent_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.lancaster import LancasterStemmer\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import KFold\n","from sklearn.preprocessing import StandardScaler\n","\n","from sklearn import metrics\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import gensim\n","from gensim.models import Word2Vec\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUDUEfCxzJH6","colab_type":"code","colab":{}},"source":["# These are the file paths where the validation/test set will be mounted (read only)\n","# into your Docker container.\n","METADATA_FILEPATH = '/usr/local/dataset/metadata.json'\n","ARTICLES_FILEPATH = '/usr/local/dataset/articles'\n","\n","# This is the filepath where the predictions should be written to.\n","PREDICTIONS_FILEPATH = '/usr/local/predictions.txt'\n","\n","# Read in the metadata file.\n","with open(METADATA_FILEPATH, 'r') as f:\n","    claims = json.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEFq6Cc20GPe","colab_type":"code","colab":{}},"source":["# extract all file paths\n","all_files = [pth for pth in Path(ARTICLES_FILEPATH).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEpS-9bI0MA-","colab_type":"code","colab":{}},"source":["# input: a list of files, output: a dictionary of articles, keys are article id, values are article string\n","def read_articles(file_list):\n","  all_articles = {}\n","  for file_path in file_list:\n","      with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","        filename = os.path.basename(file_path)\n","        filename = filename.replace(\".txt\",\"\")\n","        file_data = file.read()\n","        all_articles[filename] = file_data\n","  return all_articles"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFBEL9kx0RKd","colab_type":"code","colab":{}},"source":["# save all articles in a dictionary\n","all_articles = read_articles(all_files)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA9JgtO_0UqV","colab_type":"code","colab":{}},"source":["# input: string of article content, output: list of lines in article\n","def tosentences(article):\n","  sentence_list = sent_tokenize(article)\n","  sentence_list = list(filter(None,sentence_list))\n","\n","  return sentence_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJioVWzm0g5E","colab_type":"code","colab":{}},"source":["# input: string of article content, output: list of cleaned words\n","def cleandata(article):\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  wnLemm = WordNetLemmatizer()\n","  stop_words = set(stopwords.words('english'))\n","  \n","  article = article.replace(\"\\n\",\" \").replace(\"\\\\\",\"\")\n","  article_words = tokenizer.tokenize(article)\n","  article_words = [w.lower() for w in article_words] # add lowercase \n","  article_words = [wnLemm.lemmatize(w,'v') for w in article_words] # add lemmatizer           \n","  article_words = [w for w in article_words if not w in stop_words] # remove stop word\n","\n","  return article_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o4EdgvE0jr8","colab_type":"code","colab":{}},"source":["# input: string of claim sentence & list of sentences from article, output: tfidf dataframe\n","def tfidfsentence(article_list):\n","  claim = article_list[0]\n","  features = list(dict.fromkeys(claim))\n","  vectorizer = TfidfVectorizer(stop_words='english')\n","  X = vectorizer.fit_transform(article_list)\n","  X = X.toarray()\n","  tfidfdf = pd.DataFrame(np.round(X,2),columns=vectorizer.get_feature_names())\n","  \n","  return tfidfdf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"08v5lipe0lbk","colab_type":"code","colab":{}},"source":["# new cossim function, return the list of 5 top related sentences with highest cosine similarities\n","def top5cossim(article_list):\n","  senlist=[]\n","  cosines = []\n","  df_tfidf = tfidfsentence(article_list)\n","  claim = df_tfidf.iloc[0]\n","\n","  for index in range(1,len(article_list)):\n","    sentvec = df_tfidf.iloc[index].values\n","    cosines.append(cosine_similarity([sentvec],[claim]))\n","  cosines = np.concatenate(np.concatenate(cosines, axis=0), axis=0)\n","  cosines=pd.DataFrame(cosines)\n","  cosines.columns=['cosine']\n","  \n","  top5index = list(cosines.iloc[cosines.cosine.argsort()[::-1][:5]].index+1) # plus one because of the claim at index 0\n","  for a in top5index:\n","    senlist.append(article_list[a])\n","\n","  return senlist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7z07vTF02vs","colab_type":"code","colab":{}},"source":["# save top 5 most related sentences from related articles into a dictionary for each claims\n","related_sentences = dict.fromkeys(range(len(claims)), []) \n","for ii in range(len(claims)): # change to range(len(claims))\n","  articles = claims[ii][\"related_articles\"]\n","  allrelated = ''\n","  for articleid in articles:\n","    allrelated=allrelated+'\\n'+all_articles[str(articleid)] # read all the related articles\n","\n","  claim = claims[ii][\"claim\"]\n","  allrelated = allrelated.replace(\"\\n\",\" \").replace(\":\",\".\").replace(\";\",\".\")\n","  sentence_list=tosentences(allrelated)\n","  sentence_list.insert(0, claim)  # insert the claim to index 0\n","\n","  related_sentences[ii]=top5cossim(sentence_list) # append the list of related sentences into the dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-WUGU4_d_gk0","colab_type":"code","colab":{}},"source":["claim = []\n","index = []\n","\n","# Write the labels and corresponding claims into a list\n","for c in claims: \n","  claim.append(c['claim'])\n","for c in related_sentences: \n","  index.append(c)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lk0Jo04OzIGc","colab_type":"code","colab":{}},"source":["# Perform simple cleaning to the related sentences\n","def cleaning(article):\n","    \n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    lrStem = LancasterStemmer()\n","    wnLemm = WordNetLemmatizer()\n","    stop_words = set(stopwords.words('english'))\n","    article.replace('-',' ')\n","    \n","    article_words = tokenizer.tokenize(article)\n","    #article_words = [lrStem.stem(w) for w in article_words] # add stemmer\n","    article_words = [wnLemm.lemmatize(w,'v') for w in article_words] # add lemmatizer           \n","    article_words = [w.lower() for w in article_words if not w in stop_words] # remove stop word\n","\n","    return article_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAfDaJ9D3t7_","colab_type":"code","colab":{}},"source":["final_features = []\n","dim = 20\n","\n","# First loop across indices which represents different claims\n","for ind in index: \n","  print(\"claim: \", ind)\n","  splitted_sentences = []\n","  vocab = []\n","  sentences = []\n","  cleaned = []\n","  features = []\n","  final = []\n","\n","  # Apply the cleaning function to sentences and corresponding claim\n","  for sen in related_sentences[str(ind)]:\n","    sentences = cleaning(sen)\n","    splitted_sentences.append(sentences)\n","  \n","  cleaned = cleaning(claim[int(ind)])\n","  splitted_sentences.append(cleaned)\n","  \n","  # Create word embedding model for the splitted sentence\n","  model = Word2Vec(splitted_sentences, size=dim, window=5, min_count=1)\n","\n","  # Obtain vocabulary list\n","  vocab = list(model.wv.vocab)\n","  length = len(vocab)\n","  avg_vectors = []\n","\n","  # Record the vector representing each word in sentence and calculate the average across such sentence\n","  for i in range(0, len(splitted_sentences)):\n","    vec_sum = [0] * dim\n","    vec_count = 0\n","    for word in splitted_sentences[i]: \n","      if word in vocab: \n","        vector = list(model[str(word)])\n","        vec_sum = [sum(i) for i in zip(vec_sum, vector)]\n","        vec_count += 1\n","      else: \n","        continue\n","    if vec_count > 0: \n","      avg_vector = [x / vec_count for x in vec_sum]\n","      features.append(avg_vector)\n","    else: \n","      features.append(vec_sum)\n","\n","  if len(splitted_sentences) < 6:\n","    diff = 6 - len(splitted_sentences)\n","    for m in range(0, diff): \n","      fillin = [0] * dim\n","      features.append(fillin)\n","  # Combine all vectors under the same claim together\n","  final = np.asarray(features).flatten()\n","  # Attach the resulting overall vector in the final list as features\n","  final_features.append(final.tolist())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LlNYLKmz31py","colab_type":"code","colab":{}},"source":["X = pd.DataFrame(final_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KGw9dEb0d_Wr","colab_type":"code","colab":{}},"source":["from joblib import dump, load\n","model = load('lrmodel.joblib')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zfde9BnG3imD","colab_type":"code","colab":{}},"source":["prediction = model.predict(X)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5Ro025k4D9p","colab_type":"code","colab":{}},"source":["with open(PREDICTIONS_FILEPATH, 'a') as predictions:\n","    for p, i in zip(prediction,index):\n","        predictions.write(str(i)+','+str(p) + '\\n')\n"],"execution_count":0,"outputs":[]}]}