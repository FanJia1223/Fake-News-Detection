{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Submission File.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"8tv9D3zty2JG","colab_type":"code","colab":{}},"source":["import os\n","import re\n","import json\n","import csv\n","from pathlib import Path\n","from nltk.tokenize import RegexpTokenizer\n","from collections import Counter\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import nltk\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords \n","from nltk.tokenize import sent_tokenize\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from keras.preprocessing.sequence import pad_sequences\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import io\n","\n","%matplotlib inline\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","torch.cuda.get_device_name(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WUDUEfCxzJH6","colab_type":"code","colab":{}},"source":["# These are the file paths where the validation/test set will be mounted (read only)\n","# into your Docker container.\n","METADATA_FILEPATH = '/usr/local/dataset/metadata.json'\n","ARTICLES_FILEPATH = '/usr/local/dataset/articles'\n","\n","# This is the filepath where the predictions should be written to.\n","PREDICTIONS_FILEPATH = '/usr/local/predictions.txt'\n","\n","# Read in the metadata file.\n","with open(METADATA_FILEPATH, 'r') as f:\n","    claims = json.load(f)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEFq6Cc20GPe","colab_type":"code","colab":{}},"source":["# extract all file paths\n","all_files = [pth for pth in Path(ARTICLES_FILEPATH).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEpS-9bI0MA-","colab_type":"code","colab":{}},"source":["# input: a list of files, output: a dictionary of articles, keys are article id, values are article string\n","def read_articles(file_list):\n","  all_articles = {}\n","  for file_path in file_list:\n","      with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n","        filename = os.path.basename(file_path)\n","        filename = filename.replace(\".txt\",\"\")\n","        file_data = file.read()\n","        all_articles[filename] = file_data\n","  return all_articles"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFBEL9kx0RKd","colab_type":"code","colab":{}},"source":["# save all articles in a dictionary\n","all_articles = read_articles(all_files)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XA9JgtO_0UqV","colab_type":"code","colab":{}},"source":["# input: string of article content, output: list of lines in article\n","def tosentences(article):\n","  sentence_list = sent_tokenize(article)\n","  sentence_list = list(filter(None,sentence_list))\n","\n","  return sentence_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJioVWzm0g5E","colab_type":"code","colab":{}},"source":["# input: string of article content, output: list of cleaned words\n","def cleandata(article):\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  wnLemm = WordNetLemmatizer()\n","  stop_words = set(stopwords.words('english'))\n","  \n","  article = article.replace(\"\\n\",\" \").replace(\"\\\\\",\"\")\n","  article_words = tokenizer.tokenize(article)\n","  article_words = [w.lower() for w in article_words] # add lowercase \n","  article_words = [wnLemm.lemmatize(w,'v') for w in article_words] # add lemmatizer           \n","  article_words = [w for w in article_words if not w in stop_words] # remove stop word\n","\n","  return article_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_o4EdgvE0jr8","colab_type":"code","colab":{}},"source":["# input: string of claim sentence & list of sentences from article, output: tfidf dataframe\n","def tfidfsentence(article_list):\n","  claim = article_list[0]\n","  features = list(dict.fromkeys(claim))\n","  vectorizer = TfidfVectorizer(stop_words='english')\n","  X = vectorizer.fit_transform(article_list)\n","  X = X.toarray()\n","  tfidfdf = pd.DataFrame(np.round(X,2),columns=vectorizer.get_feature_names())\n","  \n","  return tfidfdf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"08v5lipe0lbk","colab_type":"code","colab":{}},"source":["# new cossim function, return the list of 5 top related sentences with highest cosine similarities\n","def top5cossim(article_list):\n","  senlist=[]\n","  cosines = []\n","  df_tfidf = tfidfsentence(article_list)\n","  claim = df_tfidf.iloc[0]\n","\n","  for index in range(1,len(article_list)):\n","    sentvec = df_tfidf.iloc[index].values\n","    cosines.append(cosine_similarity([sentvec],[claim]))\n","  cosines = np.concatenate(np.concatenate(cosines, axis=0), axis=0)\n","  cosines=pd.DataFrame(cosines)\n","  cosines.columns=['cosine']\n","  \n","  top5index = list(cosines.iloc[cosines.cosine.argsort()[::-1][:5]].index+1) # plus one because of the claim at index 0\n","  for a in top5index:\n","    senlist.append(article_list[a])\n","\n","  return senlist"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F7z07vTF02vs","colab_type":"code","colab":{}},"source":["# save top 5 most related sentences from related articles into a dictionary for each claims\n","related_sentences = dict.fromkeys(range(len(claims)), []) \n","for ii in range(len(claims)): # change to range(len(claims))\n","  articles = claims[ii][\"related_articles\"]\n","  allrelated = ''\n","  for articleid in articles:\n","    allrelated=allrelated+'\\n'+all_articles[str(articleid)] # read all the related articles\n","\n","  claim = claims[ii][\"claim\"]\n","  allrelated = allrelated.replace(\"\\n\",\" \").replace(\":\",\".\").replace(\";\",\".\")\n","  sentence_list=tosentences(allrelated)\n","  sentence_list.insert(0, claim)  # insert the claim to index 0\n","\n","  related_sentences[ii]=top5cossim(sentence_list) # append the list of related sentences into the dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjCRdcIL07ia","colab_type":"code","colab":{}},"source":["tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","def fromat_and_tokenize(queries):\n","    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in queries]\n","  # Tokenize with BERT tokenizer\n","    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n","    return tokenized_texts; \n","\n","def pad_sentences(texts,max_length):\n","    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in texts], maxlen=max_length, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","    return input_ids\n","\n","def create_attention_masks(input_ids):\n","    # Create attention masks\n","    attention_masks = []\n","    # Create a mask of 1s for each token followed by 0s for padding\n","    for seq in input_ids:\n","        seq_mask = [float(i>0) for i in seq]\n","        attention_masks.append(seq_mask)\n","    return attention_masks\n","def setup_dataloader(inputs,labels,batch_size):\n","    sentences = fromat_and_tokenize(inputs)\n","    input_ids = pad_sentences(sentences,MAX_LEN)\n","    attention_masks = create_attention_masks(input_ids)\n","\n","    prediction_inputs = torch.tensor(input_ids)\n","    prediction_masks = torch.tensor(attention_masks)\n","    prediction_labels = torch.tensor(labels)\n"," \n","    prediction_data = TensorDataset(prediction_inputs, prediction_masks, prediction_labels)\n","    prediction_sampler = SequentialSampler(prediction_data)\n","    prediction_dataloader = DataLoader(prediction_data, sampler=prediction_sampler, batch_size=batch_size)\n","    return prediction_dataloader \n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RA28Le851JvJ","colab_type":"code","colab":{}},"source":["labels = []\n","inputs = []\n","for claim, index in zip(claims,related_sentences):\n","    concat_sentence = claim['claim']\n","    for sentence in related_sentences[index]:\n","        concat_sentence = concat_sentence + \" \" + sentence\n","    if(len(concat_sentence) <= 512):\n","        inputs.append(concat_sentence)\n","        labels.append(claim['label'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7v7SU7j1acJ","colab_type":"code","colab":{}},"source":["inputs = inputs[800:1000]\n","labels = labels[800:1000]\n","MAX_LEN = 256"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yLMOYi_y1gNo","colab_type":"code","colab":{}},"source":["model = torch.load('states/model.pt.tar')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMzZdC5-1l2A","colab_type":"code","colab":{}},"source":["model.eval()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a-H_XgjM1mWP","colab_type":"code","colab":{}},"source":["model.cuda()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MSY6dmoT1pTX","colab_type":"code","colab":{}},"source":["dataloader = setup_dataloader(inputs,labels,4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yhE3JQDE1uYB","colab_type":"code","colab":{}},"source":["predictions , true_labels = [], []\n","# Predict \n","for batch in dataloader:\n","    # Add batch to GPU\n","    batch = tuple(t.to(device) for t in batch)\n","    # Unpack the inputs from our dataloader\n","    b_input_ids, b_input_mask, b_labels = batch\n","    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n","    with torch.no_grad():\n","    # Forward pass, calculate logit predictions\n","        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n","    # Move logits and labels to CPU\n","    logits = logits.detach().cpu().numpy()\n","    label_ids = b_labels.to('cpu').numpy()  \n","    # Store predictions and true labels\n","    predictions.append(logits)\n","    true_labels.append(label_ids)\n","\n","# Import and evaluate each test batch using Matthew's correlation coefficient\n","from sklearn.metrics import matthews_corrcoef\n","matthews_set = []\n","for i in range(len(true_labels)):\n","    matthews = matthews_corrcoef(true_labels[i],np.argmax(predictions[i], axis=1).flatten())\n","    matthews_set.append(matthews)\n","\n","# Flatten the predictions and true values for aggregate Matthew's evaluation on the whole dataset\n","flat_predictions = [item for sublist in predictions for item in sublist]\n","print(flat_predictions)\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","print(flat_predictions)\n","flat_true_labels = [item for sublist in true_labels for item in sublist]\n","print(flat_true_labels)\n","\n","print('Classification accuracy using BERT Fine Tuning: {0:0.2%}'.format(matthews_corrcoef(flat_true_labels, flat_predictions)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7juGgux2nrV","colab_type":"code","colab":{}},"source":["with open(PREDICTIONS_FILEPATH, 'w', newline='') as predictions:\n","    wr = csv.writer(predictions, quoting=csv.QUOTE_ALL)\n","    wr.writerow(flat_predictions)"],"execution_count":0,"outputs":[]}]}